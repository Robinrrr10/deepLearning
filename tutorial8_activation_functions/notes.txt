Tutorial 8: Activation Functions | Deep Learning Tutorial 8 (Tensorflow Tutorial, Keras & Python)
--------------------------------------------------------------------------------------------------
Here we can see how the activation funcations in neural network
Here we can see some of the activation functions

In previous tutorials we have seen now the activation functionals helps to increase the accuracy.
When we have the one of activation functiona like sigmoid function to predict whether the person buy the insurance of not with features like salary, education & age, activation function helps to give the value between 0 to 1. This helps to decide whether the person buy insurance or not based on values. If the value is below 0.5, person may not buy insurance. If value is more than 0.5, person will buy insurance.
Signmoid helps to decide the classification problems.
If there is no sigmoid functinal, value will be minus or plus infinity which is hard to predict the value.
That is the reason we need activation functions

Activation function mostly helps on output layer
Activation function will be used in hidden layer as well


If we remove activation layer from hidden and output layer, the output will be in the linear equation or the straight line.

Previously we have see how the linear function helps to predict the classification with the age factor to decide whether the person will buy insurance.

If we plot the scatter plot of the age and whether the preson buy the insurance or not then we can draw the line to see what splits whether the person buy insurance.
If any of the point goes beyond the limit, then it may not work.
There are chance it might miss classify the results.
That is step function. Step function is the one which gives the answer 0 or 1. Not with any decimals

In step function there are changes it might give 1 for 2 output layer. So we cannot give the correct results.
So it is always better to use sigmoid function which gives in decimals where we can give answers based on maximum values

Sigmoid equation:
signmoid(z) = 1 / 1 + e ^-z

There is another funtion called tanh which gives the value from -1 to 1
tanh equation:
tanh(z) = e^z - e^-z  /  e^z + e^-z

Gentral guidline is to use sigmoid for output layer and try to use tanh for all hidden layer

There is one issue in the sigmoid and tanh layer.
Its in derivatives and loss.
Derivatives gives how much the output changes based on each input.
When value was higher number the derivatives become 0.
This make the learning slower.
This is called vanishing gradients
Sigmoid and Tanh has this vanishing gradients problems. For that reason the learning process become slow

For solving this, we have below functions called ReLu
ReLu
ReLu(z) = max(0, x) 
Here, this this will give 0 if value is 0 or negative value.
And this will give the same number itself if value is any positive value
For hidden layer, if we are not sure which function to use, we can go with ReLu. That should be good
And the learning will be faster here
This is the most popular function. This is very light weight and computation will be faster.
ReLu also has vanishing gradient problem when value goes less than 0.

To solve this, we have one more activation function called Leaky ReLu.
Leaky ReLu(z) = max(0.1x, x)


Below 5 are the all list of activation functions
1. Step function
2. Sigmoid
3. tanh
4. ReLu
5. Leaky ReLu

Some time we need to try and see which function gives the best


Below python code for each function. this is just for our understanding
import math

def sigmoid(x):
  return 1 / (1 + math.exp(-x))
# This gives value between 0 to 1

def tanh(x):
  return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))
# This gives value between -1 to 1

def relu(x):
  return max(0, x)
# This gives value 0 or the number it self

def leaky_relu(x):
  return max(0.1 * x, x)
# This gives value between of number*0.1 or the number it self

These python code is just for our understanding. But we dont use this directly unless we use custom function.
Mostly we use readymade api or functions which are already available in tensor/keras.

Refer images and jupyter notebook for more details


------------------------------------------------
